 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/neutrinoox/projet7-scoring/blob/main/V_Maxime_2_notebook_mod%C3%A9lisation_092025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexte et objectifs",
    "",
    "Ce notebook prépare un pipeline complet pour le projet **Home Credit Default Risk** :",
    "",
    "- structurer le prétraitement reproductible (nettoyage, feature engineering, normalisation) ;",
    "- gérer les contraintes matérielles (8 Go de RAM) par un échantillonnage maîtrisé ;",
    "- entraîner plusieurs modèles de scoring (DummyClassifier, Régression Logistique, Random Forest, XGBoost) ;",
    "- comparer leurs performances selon l'AUC ROC et un score métier qui pénalise fortement les faux négatifs ;",
    "- préparer la sauvegarde du modèle sélectionné pour un déploiement API et un suivi via MLflow.",
    "",
    "L'objectif est de valider toutes les compétences liées au prétraitement, à l'équilibrage des classes et à la sélection d'un modèle supervisé robuste avant d'industrialiser le pipeline."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings",
    "from pathlib import Path",
    "",
    "import numpy as np",
    "import pandas as pd",
    "import matplotlib.pyplot as plt",
    "import seaborn as sns",
    "",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate",
    "from sklearn.compose import ColumnTransformer",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer, FunctionTransformer",
    "from sklearn.impute import SimpleImputer",
    "from sklearn.pipeline import Pipeline",
    "from sklearn.metrics import make_scorer, roc_auc_score, confusion_matrix, precision_score, recall_score, classification_report",
    "from sklearn.dummy import DummyClassifier",
    "from sklearn.linear_model import LogisticRegression",
    "from sklearn.ensemble import RandomForestClassifier",
    "",
    "warnings.filterwarnings('ignore')",
    "pd.options.display.max_columns = 150",
    "pd.options.display.float_format = '{:,.4f}'.format",
    "",
    "RANDOM_STATE = 42",
    "DATA_DIR = Path('data')",
    "TARGET_COL = 'TARGET'",
    "BUSINESS_THRESHOLD = 0.35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données brutes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "FILES = {",
    "    'application_train': 'application_train.csv',",
    "    'application_test': 'application_test.csv',",
    "    'bureau': 'bureau.csv',",
    "    'bureau_balance': 'bureau_balance.csv',",
    "    'credit_card_balance': 'credit_card_balance.csv',",
    "    'installments_payments': 'installments_payments.csv',",
    "    'POS_CASH_balance': 'POS_CASH_balance.csv',",
    "    'previous_application': 'previous_application.csv',",
    "    'HomeCredit_columns_description': 'HomeCredit_columns_description.csv',",
    "    'sample_submission': 'sample_submission.csv'",
    "}",
    "",
    "raw_data = {}",
    "for name, filename in FILES.items():",
    "    filepath = DATA_DIR / filename",
    "    if not filepath.exists():",
    "        print(f\"ATTENTION : fichier manquant {filepath}\")",
    "        continue",
    "    raw_data[name] = pd.read_csv(filepath)",
    "    print(f\"OK - {name} chargé ({raw_data[name].shape[0]} lignes, {raw_data[name].shape[1]} colonnes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profil rapide des tables utiles"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def quick_profile(df: pd.DataFrame, name: str) -> pd.DataFrame:",
    "    print(f\"===== {name.upper()} =====\")",
    "    print(f\"Shape : {df.shape[0]:,} lignes x {df.shape[1]} colonnes\")",
    "    type_counts = df.dtypes.value_counts()",
    "    print('Types de colonnes :')",
    "    print(type_counts.to_string())",
    "    missing_ratio = df.isna().mean().sort_values(ascending=False)",
    "    return missing_ratio.to_frame(name='ratio_missing')",
    "",
    "profil_application = quick_profile(raw_data['application_train'], 'application_train')",
    "profil_application.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de la variable cible"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "target_rate = raw_data['application_train'][TARGET_COL].mean()",
    "print(f\"Taux de défaut global : {target_rate:.2%}\")",
    "fig, ax = plt.subplots(figsize=(5, 3))",
    "sns.countplot(x=TARGET_COL, data=raw_data['application_train'], ax=ax)",
    "ax.set_title('Répartition de la cible TARGET')",
    "ax.bar_label(ax.containers[0], label_type='edge')",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering ciblé sur `application_train`",
    "",
    "Les transformations ci-dessous couvrent les attentes CE1 à CE4 :",
    "",
    "- nettoyage et encodage des variables catégorielles ;",
    "- création de nouvelles variables métiers (ratios, agrégations, transformations logarithmiques) ;",
    "- transformation mathématique des distributions (logarithme, ratios) ;",
    "- préparation des colonnes pour la normalisation ultérieure dans le pipeline."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_application_features(df: pd.DataFrame) -> pd.DataFrame:",
    "    df = df.copy()",
    "",
    "    df = df[df['CODE_GENDER'] != 'XNA']",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)",
    "    df['DAYS_LAST_PHONE_CHANGE'].replace(0, np.nan, inplace=True)",
    "",
    "    for col in ['DAYS_BIRTH', 'DAYS_ID_PUBLISH', 'DAYS_REGISTRATION', 'DAYS_LAST_PHONE_CHANGE']:",
    "        df[col] = df[col] * -1",
    "    df.rename(columns={",
    "        'DAYS_BIRTH': 'AGE_DAYS',",
    "        'DAYS_ID_PUBLISH': 'ID_PUBLISH_DAYS',",
    "        'DAYS_REGISTRATION': 'REGISTRATION_DAYS',",
    "        'DAYS_LAST_PHONE_CHANGE': 'LAST_PHONE_CHANGE_DAYS'",
    "    }, inplace=True)",
    "    df['YEARS_EMPLOYED'] = df['DAYS_EMPLOYED'] / -365.25",
    "    df['AGE_YEARS'] = df['AGE_DAYS'] / 365.25",
    "    df['YEARS_SINCE_REGISTRATION'] = df['REGISTRATION_DAYS'] / 365.25",
    "    df['YEARS_SINCE_ID_PUBLISH'] = df['ID_PUBLISH_DAYS'] / 365.25",
    "    df['YEARS_SINCE_PHONE_CHANGE'] = df['LAST_PHONE_CHANGE_DAYS'] / 365.25",
    "",
    "    df['CREDIT_INCOME_RATIO'] = df['AMT_CREDIT'] / (df['AMT_INCOME_TOTAL'] + 1)",
    "    df['ANNUITY_INCOME_RATIO'] = df['AMT_ANNUITY'] / (df['AMT_INCOME_TOTAL'] + 1)",
    "    df['CREDIT_ANNUITY_RATIO'] = df['AMT_CREDIT'] / (df['AMT_ANNUITY'] + 1)",
    "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / (df['CNT_FAM_MEMBERS'] + 1e-3)",
    "    df['CHILDREN_RATIO'] = df['CNT_CHILDREN'] / (df['CNT_FAM_MEMBERS'] + 1e-3)",
    "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / (df['AMT_CREDIT'] + 1)",
    "",
    "    ext_cols = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']",
    "    df['EXT_SOURCE_MEAN'] = df[ext_cols].mean(axis=1)",
    "    df['EXT_SOURCE_STD'] = df[ext_cols].std(axis=1)",
    "    df['EXT_SOURCE_MAX'] = df[ext_cols].max(axis=1)",
    "",
    "    df['LOG_INCOME_TOTAL'] = np.log1p(df['AMT_INCOME_TOTAL'])",
    "    df['LOG_CREDIT'] = np.log1p(df['AMT_CREDIT'])",
    "    df['LOG_ANNUITY'] = np.log1p(df['AMT_ANNUITY'].fillna(0))",
    "",
    "    doc_flags = df.filter(regex='^FLAG_DOCUMENT').columns",
    "    df['NB_DOCUMENTS_FOURNIS'] = df[doc_flags].sum(axis=1)",
    "",
    "    binary_map = {'Y': 1, 'N': 0}",
    "    for col in ['FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:",
    "        if df[col].dtype == 'object':",
    "            df[col] = df[col].map(binary_map)",
    "",
    "    object_cols = df.select_dtypes(include='object').columns",
    "    for col in object_cols:",
    "        df[col] = df[col].astype('category')",
    "",
    "    return df",
    "",
    "application_features = build_application_features(raw_data['application_train'])",
    "print(f\"Dataset enrichi : {application_features.shape[0]:,} lignes x {application_features.shape[1]} colonnes\")",
    "application_features.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Échantillonnage contrôlé (contrainte 8 Go RAM)",
    "",
    "On sous-échantillonne uniquement la classe majoritaire pour limiter la taille mémoire tout en conservant **tous** les clients en défaut."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def stratified_downsample(df: pd.DataFrame, target: str, max_per_class: dict[int, int], random_state: int = 42) -> pd.DataFrame:",
    "    samples = []",
    "    for label, group in df.groupby(target):",
    "        limit = max_per_class.get(int(label), len(group))",
    "        n_samples = min(len(group), limit)",
    "        samples.append(group.sample(n=n_samples, random_state=random_state))",
    "    sampled = pd.concat(samples).sample(frac=1, random_state=random_state).reset_index(drop=True)",
    "    return sampled",
    "",
    "MAX_SAMPLES = {0: 80_000, 1: 25_000}",
    "model_df = stratified_downsample(application_features, TARGET_COL, MAX_SAMPLES, random_state=RANDOM_STATE)",
    "print(model_df[TARGET_COL].value_counts())",
    "print(f\"Forme finale pour la modélisation : {model_df.shape[0]:,} lignes x {model_df.shape[1]} colonnes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Séparation apprentissage / test"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = model_df.drop(columns=[TARGET_COL])",
    "y = model_df[TARGET_COL]",
    "",
    "X_train, X_test, y_train, y_test = train_test_split(",
    "    X, y,",
    "    test_size=0.2,",
    "    stratify=y,",
    "    random_state=RANDOM_STATE",
    ")",
    "",
    "print(f\"Train : {X_train.shape}, Test : {X_test.shape}\")",
    "print(f\"Taux de défaut train : {y_train.mean():.2%} | test : {y_test.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline de prétraitement"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "numeric_cols = X_train.select_dtypes(include=['number', 'bool']).columns.tolist()",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()",
    "",
    "print(f\"Variables numériques : {len(numeric_cols)} | Variables catégorielles : {len(categorical_cols)}\")",
    "",
    "numeric_transformer = Pipeline(steps=[",
    "    ('imputer', SimpleImputer(strategy='median')),",
    "    ('yeo_johnson', PowerTransformer(method='yeo-johnson', standardize=False)),",
    "    ('scaler', StandardScaler())",
    "])",
    "",
    "categorical_transformer = Pipeline(steps=[",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))",
    "])",
    "",
    "preprocessor = ColumnTransformer(",
    "    transformers=[",
    "        ('num', numeric_transformer, numeric_cols),",
    "        ('cat', categorical_transformer, categorical_cols)",
    "    ],",
    "    remainder='drop',",
    "    sparse_threshold=0.3",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score métier et métriques d'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def business_score(y_true, y_pred_proba, threshold: float = BUSINESS_THRESHOLD) -> float:",
    "    y_pred = (y_pred_proba >= threshold).astype(int)",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()",
    "    gain_tp = 1000",
    "    cost_fp = 200",
    "    cost_fn = 2500",
    "    score = (tp * gain_tp) - (fp * cost_fp) - (fn * cost_fn)",
    "    return score / len(y_true)",
    "",
    "business_scorer = make_scorer(business_score, needs_proba=True, greater_is_better=True)",
    "",
    "scoring = {",
    "    'roc_auc': 'roc_auc',",
    "    'business': business_scorer,",
    "    'recall': 'recall',",
    "    'precision': 'precision'",
    "}",
    "",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition des modèles"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def to_dense(x):",
    "    return x.toarray() if hasattr(x, 'toarray') else x",
    "",
    "dense_transformer = FunctionTransformer(to_dense, accept_sparse=True)",
    "",
    "log_reg = Pipeline(steps=[",
    "    ('preprocess', preprocessor),",
    "    ('model', LogisticRegression(",
    "        max_iter=1000,",
    "        class_weight='balanced',",
    "        solver='lbfgs',",
    "        random_state=RANDOM_STATE",
    "    ))",
    "])",
    "",
    "rf = Pipeline(steps=[",
    "    ('preprocess', preprocessor),",
    "    ('to_dense', dense_transformer),",
    "    ('model', RandomForestClassifier(",
    "        n_estimators=300,",
    "        max_depth=None,",
    "        min_samples_split=10,",
    "        class_weight='balanced_subsample',",
    "        n_jobs=-1,",
    "        random_state=RANDOM_STATE",
    "    ))",
    "])",
    "",
    "try:",
    "    from xgboost import XGBClassifier",
    "except ImportError as exc:",
    "    raise ImportError('xgboost doit être installé pour exécuter ce notebook (pip install xgboost)') from exc",
    "",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()",
    "",
    "xgb = Pipeline(steps=[",
    "    ('preprocess', preprocessor),",
    "    ('to_dense', dense_transformer),",
    "    ('model', XGBClassifier(",
    "        objective='binary:logistic',",
    "        eval_metric='auc',",
    "        learning_rate=0.05,",
    "        n_estimators=400,",
    "        max_depth=5,",
    "        subsample=0.8,",
    "        colsample_bytree=0.8,",
    "        reg_lambda=1.0,",
    "        gamma=0.0,",
    "        min_child_weight=5,",
    "        scale_pos_weight=scale_pos_weight,",
    "        random_state=RANDOM_STATE,",
    "        n_jobs=4,",
    "        tree_method='hist'",
    "    ))",
    "])",
    "",
    "models = {",
    "    'DummyClassifier': Pipeline(steps=[('preprocess', preprocessor), ('model', DummyClassifier(strategy='most_frequent'))]),",
    "    'LogisticRegression': log_reg,",
    "    'RandomForest': rf,",
    "    'XGBoost': xgb",
    "}",
    "",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation croisée des modèles"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_models(model_dict):",
    "    rows = []",
    "    raw_results = {}",
    "    for name, model in model_dict.items():",
    "        print(f\"\nEvaluation de {name}\")",
    "        cv_results = cross_validate(",
    "            model,",
    "            X_train,",
    "            y_train,",
    "            cv=cv,",
    "            scoring=scoring,",
    "            n_jobs=-1,",
    "            return_estimator=False",
    "        )",
    "        raw_results[name] = cv_results",
    "        rows.append({",
    "            'Modèle': name,",
    "            'AUC moyenne': cv_results['test_roc_auc'].mean(),",
    "            'AUC écart-type': cv_results['test_roc_auc'].std(),",
    "            'Score métier moyen': cv_results['test_business'].mean(),",
    "            'Score métier écart-type': cv_results['test_business'].std(),",
    "            'Recall moyen': cv_results['test_recall'].mean(),",
    "            'Précision moyenne': cv_results['test_precision'].mean()",
    "        })",
    "    summary = pd.DataFrame(rows).sort_values('Score métier moyen', ascending=False).reset_index(drop=True)",
    "    return summary, raw_results",
    "",
    "cv_summary, cv_details = evaluate_models(models)",
    "cv_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Évaluation sur l'ensemble de test"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_model_name = cv_summary.loc[0, 'Modèle']",
    "print(f\"Modèle le plus prometteur selon le score métier : {best_model_name}\")",
    "",
    "best_model = models[best_model_name]",
    "best_model.fit(X_train, y_train)",
    "",
    "test_proba = best_model.predict_proba(X_test)[:, 1]",
    "auc_test = roc_auc_score(y_test, test_proba)",
    "",
    "test_pred = (test_proba >= BUSINESS_THRESHOLD).astype(int)",
    "conf_mat = confusion_matrix(y_test, test_pred)",
    "recall_test = recall_score(y_test, test_pred)",
    "precision_test = precision_score(y_test, test_pred)",
    "business_test = business_score(y_test, test_proba)",
    "",
    "print(f\"AUC test : {auc_test:.4f}\")",
    "print(f\"Score métier test : {business_test:.2f}\")",
    "print(f\"Recall test : {recall_test:.2%} | Précision test : {precision_test:.2%}\")",
    "print(\"\nMatrice de confusion :\n\", conf_mat)",
    "print(\"\nClassification report :\n\", classification_report(y_test, test_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation à l'industrialisation",
    "",
    "Le pipeline entraîné (`best_model`) peut désormais être :",
    "",
    "1. enregistré via `mlflow.sklearn.log_model` lors d'une exécution instrumentée ;",
    "2. sérialisé localement (`joblib.dump`) pour l'intégration dans l'API FastAPI / Streamlit ;",
    "3. comparé aux prochains modèles (LightGBM, CatBoost, etc.) dans un registre de modèles.",
    "",
    "La sélection finale du modèle et son suivi MLflow seront réalisés dans une itération dédiée."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "machine_shape": "hm",
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0