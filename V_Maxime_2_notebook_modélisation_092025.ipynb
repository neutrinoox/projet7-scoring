  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<a href=\"https://colab.research.google.com/github/neutrinoox/projet7-scoring/blob/main/V_Maxime_2_notebook_mod%C3%A9lisation_092025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Contexte et objectifs\n\nCe notebook prépare un pipeline complet pour le projet **Home Credit Default Risk** :\n\n- structurer le prétraitement reproductible (nettoyage, feature engineering, normalisation) ;\n- gérer les contraintes matérielles (8 Go de RAM) par un échantillonnage maîtrisé ;\n- entraîner plusieurs modèles de scoring (DummyClassifier, Régression Logistique, Random Forest, XGBoost) ;\n- comparer leurs performances selon l'AUC ROC et un score métier qui pénalise fortement les faux négatifs ;\n- préparer la sauvegarde du modèle sélectionné pour un déploiement API et un suivi via MLflow.\n\nL'objectif est de valider toutes les compétences liées au prétraitement, à l'équilibrage des classes et à la sélection d'un modèle supervisé robuste avant d'industrialiser le pipeline.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer, FunctionTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import make_scorer, roc_auc_score, confusion_matrix, precision_score, recall_score, classification_report\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = 150\npd.options.display.float_format = '{:,.4f}'.format\n\nRANDOM_STATE = 42\nDATA_DIR = Path('data')\nTARGET_COL = 'TARGET'\nBUSINESS_THRESHOLD = 0.35\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Chargement des données brutes\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "FILES = {\n    'application_train': 'application_train.csv',\n    'application_test': 'application_test.csv',\n    'bureau': 'bureau.csv',\n    'bureau_balance': 'bureau_balance.csv',\n    'credit_card_balance': 'credit_card_balance.csv',\n    'installments_payments': 'installments_payments.csv',\n    'POS_CASH_balance': 'POS_CASH_balance.csv',\n    'previous_application': 'previous_application.csv',\n    'HomeCredit_columns_description': 'HomeCredit_columns_description.csv',\n    'sample_submission': 'sample_submission.csv'\n}\n\nraw_data = {}\nfor name, filename in FILES.items():\n    filepath = DATA_DIR / filename\n    if not filepath.exists():\n        print(f\"ATTENTION : fichier manquant {filepath}\")\n        continue\n    raw_data[name] = pd.read_csv(filepath)\n    print(f\"OK - {name} chargé ({raw_data[name].shape[0]} lignes, {raw_data[name].shape[1]} colonnes)\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Profil rapide des tables utiles\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def quick_profile(df: pd.DataFrame, name: str) -> pd.DataFrame:\n    print(f\"===== {name.upper()} =====\")\n    print(f\"Shape : {df.shape[0]:,} lignes x {df.shape[1]} colonnes\")\n    type_counts = df.dtypes.value_counts()\n    print('Types de colonnes :')\n    print(type_counts.to_string())\n    missing_ratio = df.isna().mean().sort_values(ascending=False)\n    return missing_ratio.to_frame(name='ratio_missing')\n\nprofil_application = quick_profile(raw_data['application_train'], 'application_train')\nprofil_application.head(15)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Analyse de la variable cible\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "target_rate = raw_data['application_train'][TARGET_COL].mean()\nprint(f\"Taux de défaut global : {target_rate:.2%}\")\nfig, ax = plt.subplots(figsize=(5, 3))\nsns.countplot(x=TARGET_COL, data=raw_data['application_train'], ax=ax)\nax.set_title('Répartition de la cible TARGET')\nax.bar_label(ax.containers[0], label_type='edge')\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Feature engineering ciblé sur `application_train`\n\nLes transformations ci-dessous couvrent les attentes CE1 à CE4 :\n\n- nettoyage et encodage des variables catégorielles ;\n- création de nouvelles variables métiers (ratios, agrégations, transformations logarithmiques) ;\n- transformation mathématique des distributions (logarithme, ratios) ;\n- préparation des colonnes pour la normalisation ultérieure dans le pipeline.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def build_application_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    df = df[df['CODE_GENDER'] != 'XNA']\n    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n    df['DAYS_LAST_PHONE_CHANGE'].replace(0, np.nan, inplace=True)\n\n    for col in ['DAYS_BIRTH', 'DAYS_ID_PUBLISH', 'DAYS_REGISTRATION', 'DAYS_LAST_PHONE_CHANGE']:\n        df[col] = df[col] * -1\n    df.rename(columns={\n        'DAYS_BIRTH': 'AGE_DAYS',\n        'DAYS_ID_PUBLISH': 'ID_PUBLISH_DAYS',\n        'DAYS_REGISTRATION': 'REGISTRATION_DAYS',\n        'DAYS_LAST_PHONE_CHANGE': 'LAST_PHONE_CHANGE_DAYS'\n    }, inplace=True)\n    df['YEARS_EMPLOYED'] = df['DAYS_EMPLOYED'] / -365.25\n    df['AGE_YEARS'] = df['AGE_DAYS'] / 365.25\n    df['YEARS_SINCE_REGISTRATION'] = df['REGISTRATION_DAYS'] / 365.25\n    df['YEARS_SINCE_ID_PUBLISH'] = df['ID_PUBLISH_DAYS'] / 365.25\n    df['YEARS_SINCE_PHONE_CHANGE'] = df['LAST_PHONE_CHANGE_DAYS'] / 365.25\n\n    df['CREDIT_INCOME_RATIO'] = df['AMT_CREDIT'] / (df['AMT_INCOME_TOTAL'] + 1)\n    df['ANNUITY_INCOME_RATIO'] = df['AMT_ANNUITY'] / (df['AMT_INCOME_TOTAL'] + 1)\n    df['CREDIT_ANNUITY_RATIO'] = df['AMT_CREDIT'] / (df['AMT_ANNUITY'] + 1)\n    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / (df['CNT_FAM_MEMBERS'] + 1e-3)\n    df['CHILDREN_RATIO'] = df['CNT_CHILDREN'] / (df['CNT_FAM_MEMBERS'] + 1e-3)\n    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / (df['AMT_CREDIT'] + 1)\n\n    ext_cols = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']\n    df['EXT_SOURCE_MEAN'] = df[ext_cols].mean(axis=1)\n    df['EXT_SOURCE_STD'] = df[ext_cols].std(axis=1)\n    df['EXT_SOURCE_MAX'] = df[ext_cols].max(axis=1)\n\n    df['LOG_INCOME_TOTAL'] = np.log1p(df['AMT_INCOME_TOTAL'])\n    df['LOG_CREDIT'] = np.log1p(df['AMT_CREDIT'])\n    df['LOG_ANNUITY'] = np.log1p(df['AMT_ANNUITY'].fillna(0))\n\n    doc_flags = df.filter(regex='^FLAG_DOCUMENT').columns\n    df['NB_DOCUMENTS_FOURNIS'] = df[doc_flags].sum(axis=1)\n\n    binary_map = {'Y': 1, 'N': 0}\n    for col in ['FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n        if df[col].dtype == 'object':\n            df[col] = df[col].map(binary_map)\n\n    object_cols = df.select_dtypes(include='object').columns\n    for col in object_cols:\n        df[col] = df[col].astype('category')\n\n    return df\n\napplication_features = build_application_features(raw_data['application_train'])\nprint(f\"Dataset enrichi : {application_features.shape[0]:,} lignes x {application_features.shape[1]} colonnes\")\napplication_features.head(3)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Échantillonnage contrôlé (contrainte 8 Go RAM)\n\nOn sous-échantillonne uniquement la classe majoritaire pour limiter la taille mémoire tout en conservant **tous** les clients en défaut.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def stratified_downsample(df: pd.DataFrame, target: str, max_per_class: dict[int, int], random_state: int = 42) -> pd.DataFrame:\n    samples = []\n    for label, group in df.groupby(target):\n        limit = max_per_class.get(int(label), len(group))\n        n_samples = min(len(group), limit)\n        samples.append(group.sample(n=n_samples, random_state=random_state))\n    sampled = pd.concat(samples).sample(frac=1, random_state=random_state).reset_index(drop=True)\n    return sampled\n\nMAX_SAMPLES = {0: 80_000, 1: 25_000}\nmodel_df = stratified_downsample(application_features, TARGET_COL, MAX_SAMPLES, random_state=RANDOM_STATE)\nprint(model_df[TARGET_COL].value_counts())\nprint(f\"Forme finale pour la modélisation : {model_df.shape[0]:,} lignes x {model_df.shape[1]} colonnes\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Séparation apprentissage / test\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "X = model_df.drop(columns=[TARGET_COL])\ny = model_df[TARGET_COL]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    stratify=y,\n    random_state=RANDOM_STATE\n)\n\nprint(f\"Train : {X_train.shape}, Test : {X_test.shape}\")\nprint(f\"Taux de défaut train : {y_train.mean():.2%} | test : {y_test.mean():.2%}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Pipeline de prétraitement\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "numeric_cols = X_train.select_dtypes(include=['number', 'bool']).columns.tolist()\ncategorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n\nprint(f\"Variables numériques : {len(numeric_cols)} | Variables catégorielles : {len(categorical_cols)}\")\n\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('yeo_johnson', PowerTransformer(method='yeo-johnson', standardize=False)),\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ],\n    remainder='drop',\n    sparse_threshold=0.3\n)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Score métier et métriques d'évaluation\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def business_score(y_true, y_pred_proba, threshold: float = BUSINESS_THRESHOLD) -> float:\n    y_pred = (y_pred_proba >= threshold).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    gain_tp = 1000\n    cost_fp = 200\n    cost_fn = 2500\n    score = (tp * gain_tp) - (fp * cost_fp) - (fn * cost_fn)\n    return score / len(y_true)\n\nbusiness_scorer = make_scorer(business_score, needs_proba=True, greater_is_better=True)\n\nscoring = {\n    'roc_auc': 'roc_auc',\n    'business': business_scorer,\n    'recall': 'recall',\n    'precision': 'precision'\n}\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Définition des modèles\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def to_dense(x):\n    return x.toarray() if hasattr(x, 'toarray') else x\n\ndense_transformer = FunctionTransformer(to_dense, accept_sparse=True)\n\nlog_reg = Pipeline(steps=[\n    ('preprocess', preprocessor),\n    ('model', LogisticRegression(\n        max_iter=1000,\n        class_weight='balanced',\n        solver='lbfgs',\n        random_state=RANDOM_STATE\n    ))\n])\n\nrf = Pipeline(steps=[\n    ('preprocess', preprocessor),\n    ('to_dense', dense_transformer),\n    ('model', RandomForestClassifier(\n        n_estimators=300,\n        max_depth=None,\n        min_samples_split=10,\n        class_weight='balanced_subsample',\n        n_jobs=-1,\n        random_state=RANDOM_STATE\n    ))\n])\n\ntry:\n    from xgboost import XGBClassifier\nexcept ImportError as exc:\n    raise ImportError('xgboost doit être installé pour exécuter ce notebook (pip install xgboost)') from exc\n\nscale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n\nxgb = Pipeline(steps=[\n    ('preprocess', preprocessor),\n    ('to_dense', dense_transformer),\n    ('model', XGBClassifier(\n        objective='binary:logistic',\n        eval_metric='auc',\n        learning_rate=0.05,\n        n_estimators=400,\n        max_depth=5,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        reg_lambda=1.0,\n        gamma=0.0,\n        min_child_weight=5,\n        scale_pos_weight=scale_pos_weight,\n        random_state=RANDOM_STATE,\n        n_jobs=4,\n        tree_method='hist'\n    ))\n])\n\nmodels = {\n    'DummyClassifier': Pipeline(steps=[('preprocess', preprocessor), ('model', DummyClassifier(strategy='most_frequent'))]),\n    'LogisticRegression': log_reg,\n    'RandomForest': rf,\n    'XGBoost': xgb\n}\n\nmodels\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Validation croisée des modèles\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def evaluate_models(model_dict):\n    rows = []\n    raw_results = {}\n    for name, model in model_dict.items():\n        print(f\"\nEvaluation de {name}\")\n        cv_results = cross_validate(\n            model,\n            X_train,\n            y_train,\n            cv=cv,\n            scoring=scoring,\n            n_jobs=-1,\n            return_estimator=False\n        )\n        raw_results[name] = cv_results\n        rows.append({\n            'Modèle': name,\n            'AUC moyenne': cv_results['test_roc_auc'].mean(),\n            'AUC écart-type': cv_results['test_roc_auc'].std(),\n            'Score métier moyen': cv_results['test_business'].mean(),\n            'Score métier écart-type': cv_results['test_business'].std(),\n            'Recall moyen': cv_results['test_recall'].mean(),\n            'Précision moyenne': cv_results['test_precision'].mean()\n        })\n    summary = pd.DataFrame(rows).sort_values('Score métier moyen', ascending=False).reset_index(drop=True)\n    return summary, raw_results\n\ncv_summary, cv_details = evaluate_models(models)\ncv_summary\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Évaluation sur l'ensemble de test\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "best_model_name = cv_summary.loc[0, 'Modèle']\nprint(f\"Modèle le plus prometteur selon le score métier : {best_model_name}\")\n\nbest_model = models[best_model_name]\nbest_model.fit(X_train, y_train)\n\ntest_proba = best_model.predict_proba(X_test)[:, 1]\nauc_test = roc_auc_score(y_test, test_proba)\n\ntest_pred = (test_proba >= BUSINESS_THRESHOLD).astype(int)\nconf_mat = confusion_matrix(y_test, test_pred)\nrecall_test = recall_score(y_test, test_pred)\nprecision_test = precision_score(y_test, test_pred)\nbusiness_test = business_score(y_test, test_proba)\n\nprint(f\"AUC test : {auc_test:.4f}\")\nprint(f\"Score métier test : {business_test:.2f}\")\nprint(f\"Recall test : {recall_test:.2%} | Précision test : {precision_test:.2%}\")\nprint(\"\nMatrice de confusion :\n\", conf_mat)\nprint(\"\nClassification report :\n\", classification_report(y_test, test_pred, digits=3))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Préparation à l'industrialisation\n\nLe pipeline entraîné (`best_model`) peut désormais être :\n\n1. enregistré via `mlflow.sklearn.log_model` lors d'une exécution instrumentée ;\n2. sérialisé localement (`joblib.dump`) pour l'intégration dans l'API FastAPI / Streamlit ;\n3. comparé aux prochains modèles (LightGBM, CatBoost, etc.) dans un registre de modèles.\n\nLa sélection finale du modèle et son suivi MLflow seront réalisés dans une itération dédiée.\n"
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}